{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "count = 0\n",
    "val_names = [] \n",
    "\n",
    "# get the list of all images in the validation set  \n",
    "for dirname, _, filenames in os.walk('/kaggle/input/ifood-data/val_set'):\n",
    "    for filename in filenames:\n",
    "        val_names.append(filename)\n",
    "        # print(os.path.join(dirname, filename))\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = [] \n",
    "# get the list of all images in the training set \n",
    "for dirname, _, filenames in os.walk('/kaggle/input/ifood-data/train_set'):\n",
    "    for filename in filenames:\n",
    "        # print(os.path.join(dirname, filename))\n",
    "        train_names.append(filename)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "NUM_TRAIN = 0\n",
    "NUM_VAL = 0\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. \n",
    "    Extendstorchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"Custom dataset object for loading images and image labels\n",
    "    Extends torch.utils.data.Dataset\n",
    "    -----------\n",
    "    image_path: location of the folder where the images are stored \n",
    "    image_names: list of all of the image files at the location \n",
    "    label_path: the path to the csv of the corresponding label to each image\n",
    "    \"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "    def __init__(self, \n",
    "                 image_path, \n",
    "                 image_names, \n",
    "                 label_path,  transform=None):\n",
    "\n",
    "        self.image_path = image_path \n",
    "        self.image_names = image_names \n",
    "        self.transform = transform\n",
    "        # reads a csv of all the labels \n",
    "        self.labels = pd.read_csv(label_path, \n",
    "                                  names=[\"img_name\", \"label\"])\n",
    "        \n",
    "    def get_class_label(self, image_name):\n",
    "        y = self.labels[self.labels[\"img_name\"] == image_name].iloc[0][\"label\"]\n",
    "        return y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_path + \"/\" + self.image_names[index]\n",
    "        x = Image.open(path)\n",
    "        y = self.get_class_label(path.split('/')[-1])\n",
    "        \n",
    "        # apply transformations \n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "# the following will apply a list of transformations to the images\n",
    "# transformations for the training set \n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.768986940,.6641706 ,0.5923363),\n",
    "                         (0.18613161, 0.22524446, 0.23932885))\n",
    "])\n",
    "\n",
    "# transformations for the validation set \n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.768986940,.6641706 ,0.5923363),\n",
    "                         (0.18613161, 0.22524446, 0.23932885))\n",
    "])\n",
    "\n",
    "# transformations for the test set \n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.768986940,.6641706 ,0.5923363),\n",
    "                         (0.18613161, 0.22524446, 0.23932885))\n",
    "])    \n",
    "\n",
    "# create dataset for training, validation images with the correct labels\n",
    "# use of the path variable assumes that train_set, val_set and test_set are \n",
    "# all stored at the same location \n",
    "\n",
    "path = \"/kaggle/input/ifood-data/\" \n",
    "\n",
    "train_data = MyDataset(path +\"train_set/train_set\", # location of the train_set\n",
    "                       train_names, # list of image_names \n",
    "                       \"/kaggle/input/ifood-rice/train_info.csv\", # csv for labels \n",
    "                       transform= train_transforms) # apply the transformations \n",
    "\n",
    "val_data = MyDataset(path +\"val_set/val_set\", \n",
    "                     val_names, \n",
    "                     \"/kaggle/input/ifood-rice/val_info.csv\", \n",
    "                     transform= val_transforms)\n",
    "\n",
    "# load testing images from path \n",
    "test_data = ImageFolderWithPaths(path + \"test_set\", \n",
    "                                 transform = test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # edit to change batch size \n",
    "\n",
    "# create data loader objects for batch gradient descent \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load an image one at a time for test set \n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True # set this to False if you want to use CPU \n",
    "\n",
    "dtype = torch.float32 \n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 11800\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    \"\"\"calculates model accuracy on data loader \n",
    "    ----------\n",
    "    loader: the dataset to check on, e.g train_loader, val_loader, test_loader\n",
    "    model: a pytorch deeplearning model\n",
    "    ----------\n",
    "    output: prints the accuracy \n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            # idenitfy the group with the highest value \n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        \n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "def train(model, optimizer, epochs):\n",
    "    \"\"\"performs gradient descent with optimizer \n",
    "    ------------\n",
    "    model: a pytorch model \n",
    "    optimizer: optimizer from pytorch.optim \n",
    "    epochs: the number of epochs to train for \n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                print(t, loss.item())\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(val_loader, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a previously trained model from path \n",
    "model = torch.load(\"/kaggle/input/workingresnet152-trainpth/resnet152_train4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can edit the parameters of the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=.0005, momentum=.9, nesterov=True)\n",
    "train(model, optimizer, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy \n",
    "check_accuracy(val_loader, model)\n",
    "model.eval()\n",
    "model.to(device=device)\n",
    "\n",
    "# load testing images and write predicted labels into a txt file \n",
    "with open('submission_thurs1.txt', 'w') as file:\n",
    "    file.write(\"img_name,label\\n\")\n",
    "    with torch.no_grad():\n",
    "        for x, y, path in test_loader:\n",
    "            name = path[0][-15:]\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            scores = model(x)\n",
    "            out_labels = [int(x) for x in (torch.topk(scores, 3)[1][0])]\n",
    "            file.write(name + \",\" + str(out_labels[0]) + \" \" + str(out_labels[1]) + \" \" + str(out_labels[2]) + \"\\n\")\n",
    "print(\"done writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/kaggle/working/resnet152_10epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
